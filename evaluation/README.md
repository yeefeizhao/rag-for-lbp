# Evaluation Module

This folder contains scripts and data for evaluating the quality, readability, and inter-rater reliability of patient education materials generated by different RAG model configurations for lower back pain management.

## Overview

The evaluation module assesses generated patient education materials across multiple dimensions:
- **Content Quality**: Redundancy, accuracy, and completeness (expert evaluations)
- **Readability**: Flesch-Kincaid readability scores and grade levels
- **Statistical Analysis**: ANOVA, ICC (inter-rater reliability), standard deviation analysis
- **Visualization**: Radar plots comparing model performance across metrics

## Files

### Evaluation Scripts

#### `evaluation_metrics.py`
Combines and aggregates expert evaluations from two independent evaluators.

**Purpose:**
- Merges evaluation data from James Hill and Dave Thompson
- Calculates mean and standard deviation for each model across three quality metrics
- Generates summary statistics for model comparison

**Metrics Aggregated:**
- **Redundancy**: Degree of unnecessary repetition in generated content
- **Accuracy**: Correctness of medical information and recommendations
- **Completeness**: Coverage of relevant information for the patient profile

**Input Files:**
- `EVAL James Hill PEM Evaluation - education_materials_test.csv`
- `EVAL Dave Thompson PEM Evaluation - education_materials_test.csv`

**Output:**
- `combined_model_statistics.csv` - Mean and SD for each model configuration

**Usage:**
```bash
python evaluation_metrics.py
```

#### `readability_test.py`
Analyzes readability of generated patient education materials using Flesch-Kincaid metrics.

**Purpose:**
- Calculates Flesch Reading Ease scores for all generated materials
- Determines Flesch-Kincaid Grade Level (calculated and permuted)
- Extracts text statistics (word count, syllables, sentences)

**Flesch Reading Ease Scale:**
- 100-90: 5th grade (Very easy to read)
- 90-80: 6th grade (Easy to read - conversational)
- 80-70: 7th grade (Fairly easy to read)
- 70-60: 8th-9th grade (Plain English)
- 60-50: 10th-12th grade (Fairly difficult)
- 50-30: College level (Difficult)
- 30-10: College graduate (Very difficult)
- 10-0: Professional (Extremely difficult)

**Flesch-Kincaid Grade Level Formula:**
```
(0.39 × words/sentences) + (11.8 × syllables/words) - 15.59
```

**Input:**
- `final_education_materials.csv` - Generated education materials from all models

**Output:**
- `raw_data_readability_output.csv` - Detailed readability metrics for each response

**Dependencies:**
- `fkscore` library for readability calculations

**Usage:**
```bash
python readability_test.py
```

#### `readability_metrics.py`
Aggregates readability scores by model configuration and calculates averages.

**Purpose:**
- Groups readability data by model configuration
- Calculates average Flesch-Kincaid scores across all patient responses
- Determines average grade level and text statistics

**Input:**
- `readability_output.csv` - Raw readability data

**Output:**
- `model_fk_averages.csv` - Average readability metrics per model

**Metrics Calculated:**
- Average FK Readability Score
- Average FK Grade Level (permuted from table)
- Average FK Calculated Grade Level
- Average word count
- Average syllable count
- Average sentence count

**Usage:**
```bash
python readability_metrics.py
```

#### `anova.py`
Performs Analysis of Variance (ANOVA) to test for significant differences between model configurations.

**Purpose:**
- Tests whether different models produce significantly different quality scores
- Evaluates statistical significance across redundancy, accuracy, and completeness
- Combines data from both evaluators for robust analysis

**Statistical Method:**
- One-way ANOVA using Ordinary Least Squares (OLS)
- Type II sum of squares
- F-statistic and p-value calculation

**Hypothesis:**
- **H₀**: No significant difference between model means
- **H₁**: At least one model differs significantly

**Input:**
- `EVAL James Hill PEM Evaluation - education_materials_test.csv`
- `EVAL Dave Thompson PEM Evaluation - education_materials_test.csv`

**Output:**
- Console output of ANOVA tables for each metric

**Dependencies:**
- `pandas`, `statsmodels`

**Usage:**
```bash
python anova.py
```

#### `icc.py`
Calculates Intraclass Correlation Coefficient (ICC) to assess inter-rater reliability between evaluators.

**Purpose:**
- Measures agreement between two independent evaluators
- Assesses reliability of quality metric scoring
- Identifies consistency in redundancy, accuracy, and completeness ratings

**ICC Interpretation:**
- < 0.50: Poor reliability
- 0.50-0.75: Moderate reliability
- 0.75-0.90: Good reliability
- > 0.90: Excellent reliability

**Input:**
- `EVAL James Hill PEM Evaluation - education_materials_test.csv`
- `EVAL Dave Thompson PEM Evaluation - education_materials_test.csv`

**Output:**
- Console output of ICC values with confidence intervals for each metric

**Dependencies:**
- `pandas`, `pingouin`

**Usage:**
```bash
python icc.py
```

#### `evaluator_sd.py`
Calculates standard deviation for each evaluator separately to assess scoring consistency.

**Purpose:**
- Measures within-evaluator variability
- Identifies if one evaluator has more consistent scoring patterns
- Helps interpret differences in evaluation approaches

**Input:**
- `james_hill_evaluation.csv`
- `dave_thompson_evaluation.csv`

**Output:**
- `evaluator_std.csv` - Standard deviations for each evaluator across all metrics

**Usage:**
```bash
python evaluator_sd.py
```

#### `radar_plot.py`
Creates interactive radar/spider plots visualizing model performance across multiple dimensions.

**Purpose:**
- Visual comparison of model configurations
- Multi-dimensional performance assessment
- Interactive exploration of trade-offs between metrics

**Dimensions Plotted:**
- Redundancy (scale: 0-86)
- Accuracy (scale: 0-74)
- Completeness (scale: 0-61)
- Readability (scale: 0-104)

**Visualizations:**
- Combined plot with all model configurations
- Normalized values (0-1 scale) for fair comparison
- Color-coded traces for each model variant

**Models Compared:**
- GPT-4 (NRAG, RAGFS, RAGNFS)
- GPT-4o (NRAG, RAGFS)
- GPT-4o-mini (NRAG, RAGFS, RAGNFS)
- GPT-3.5-turbo (RAGFS, RAGNFS)

**Output:**
- Interactive HTML plot (opens in browser)

**Dependencies:**
- `plotly`

**Usage:**
```bash
python radar_plot.py
```

**Color Scheme:**
- GPT-4_NRAG: Blue (RGB 100, 110, 250)
- GPT-4_RAGFS: Red-Orange (RGB 240, 87, 60)
- GPT-4_RAGNFS: Green (RGB 0, 204, 150)
- GPT-4O_NRAG: Purple (RGB 172, 100, 250)
- GPT-4O_RAGFS: Orange (RGB 255, 161, 89)
- GPT-4O-MINI_NRAG: Cyan (RGB 24, 210, 242)
- GPT-4O-MINI_RAGFS: Pink (RGB 255, 102, 145)
- GPT-4O-MINI_RAGNFS: Light Green (RGB 182, 232, 128)
- GPT-3.5-TURBO_RAGFS: Magenta (RGB 255, 150, 255)
- GPT-3.5-TURBO_RAGNFS: Yellow (RGB 255, 203, 82)

## Data Files

### Input Data

#### `final_education_materials.csv` / `final_education_materials.xlsx`
Complete dataset of generated patient education materials from all model configurations tested.

**Structure:**
- Model: Model configuration identifier (e.g., "GPT-4_RAGFS")
- Question: Patient profile (synthetic patient characteristics)
- Answer: Generated education material

**Patient Count:** 30 synthetic patient profiles

#### `sample_education_materials.csv`
Subset of education materials used for preliminary testing or examples.

#### `james_hill_evaluation.csv` / `dave_thompson_evaluation.csv`
Expert evaluations from two independent evaluators (physical therapy domain experts).

**Evaluation Criteria:**
- **Redundancy** (0-100 scale): Lower is better (less repetition)
- **Accuracy** (0-100 scale): Higher is better (more medically correct)
- **Completeness** (0-100 scale): Higher is better (covers all relevant topics)

**Structure:**
- Model: Configuration tested
- Patient: Patient identifier
- Redundancy: Redundancy score
- Accuracy: Accuracy score
- Completeness: Completeness score

**Evaluators:**
- **James Hill**: Physical therapist with expertise in LBP management
- **Dave Thompson**: Physical therapist with clinical research experience

### Output Data

#### `outputs/` Directory

##### `combined_model_statistics.csv`
Aggregated statistics combining both evaluators' assessments.

**Columns:**
- Model: Configuration name
- redundancy_mean: Average redundancy across evaluators
- redundancy_sd: Standard deviation of redundancy
- accuracy_mean: Average accuracy
- accuracy_sd: Standard deviation of accuracy
- completeness_mean: Average completeness
- completeness_sd: Standard deviation of completeness

##### `model_fk_averages.csv` / `model_fk_averages.xlsx`
Average readability metrics for each model configuration.

**Columns:**
- Model: Configuration name
- Avg_FK_Readability: Mean Flesch Reading Ease score
- Avg_FK_Grade: Mean grade level (permuted from table)
- Avg_FK_CalcGrade: Mean calculated grade level
- Avg_Num_Words: Mean word count per response
- Avg_Num_Syllables: Mean syllable count
- Avg_Num_Sentences: Mean sentence count

##### `raw_data_readability_output.csv`
Detailed readability scores for every individual generated response.

##### `readability_output.csv`
Processed readability data ready for analysis.

##### `evaluator_std.csv`
Standard deviations for each evaluator separately.

##### `overall_evaluator_variance.csv`
Variance analysis comparing evaluator consistency.

## Results Directory

### `results/`
Contains organized subdirectories with different experimental runs:

#### `30_patients/`
Full evaluation results with 30 synthetic patient profiles (primary dataset).

#### `4_patients/`
Pilot study results with 4 patient profiles for initial testing.

#### `4o/`
Results specific to GPT-4o model variants.

#### `without_nrag_history/`
Results from experiments excluding conversational history in non-RAG configurations.

## Evaluation Workflow

### 1. Generate Education Materials
Run RAG system from `../langchain_testing/` to create patient education materials:
```bash
cd ../langchain_testing
python testing.py
```

### 2. Expert Evaluation
Two independent evaluators manually review generated materials and score on:
- Redundancy (minimization goal)
- Accuracy (maximization goal)
- Completeness (maximization goal)

**Evaluation Process:**
- Blind evaluation (evaluators unaware of model identities)
- Standardized rubric for scoring
- Independent assessments (no collaboration)

### 3. Readability Analysis
Calculate Flesch-Kincaid metrics:
```bash
python readability_test.py
python readability_metrics.py
```

### 4. Statistical Analysis
Assess model differences and inter-rater reliability:
```bash
python evaluation_metrics.py  # Aggregate scores
python anova.py               # Test for significant differences
python icc.py                 # Inter-rater reliability
python evaluator_sd.py        # Evaluator consistency
```

### 5. Visualization
Generate comparative radar plots:
```bash
python radar_plot.py
```

## Key Metrics

### Content Quality Metrics

#### Redundancy
- **Definition**: Unnecessary repetition of information within the education material
- **Scale**: 0-100 (lower is better)
- **Ideal Range**: < 30 (minimal redundancy)
- **Evaluation**: Manual assessment of repeated concepts, phrases, or recommendations

#### Accuracy
- **Definition**: Medical correctness and appropriateness of recommendations
- **Scale**: 0-100 (higher is better)
- **Ideal Range**: > 70 (highly accurate)
- **Evaluation**: Alignment with clinical practice guidelines and evidence-based medicine

#### Completeness
- **Definition**: Coverage of relevant topics for the patient's specific profile
- **Scale**: 0-100 (higher is better)
- **Ideal Range**: > 60 (comprehensive coverage)
- **Evaluation**: Presence of key elements (exercise, ergonomics, pain management, etc.)

### Readability Metrics

#### Flesch Reading Ease (FRE)
- **Formula**: 206.835 - (1.015 × words/sentences) - (84.6 × syllables/words)
- **Scale**: 0-100 (higher is easier)
- **Target**: 80-90 (6th grade level for health literacy)

#### Flesch-Kincaid Grade Level (FKGL)
- **Formula**: (0.39 × words/sentences) + (11.8 × syllables/words) - 15.59
- **Scale**: U.S. grade level
- **Target**: 6th grade (per prompt instructions)

## Statistical Methods

### ANOVA (Analysis of Variance)
- **Purpose**: Test if model configurations produce significantly different scores
- **Method**: One-way ANOVA with Type II sum of squares
- **Null Hypothesis**: All models have equal means
- **Significance Level**: α = 0.05

### ICC (Intraclass Correlation Coefficient)
- **Purpose**: Measure agreement between evaluators
- **Type**: Two-way mixed effects, absolute agreement
- **Interpretation**: Values closer to 1 indicate higher reliability

### Standard Deviation Analysis
- **Purpose**: Assess scoring consistency within and between evaluators
- **Application**: Identify systematic differences in evaluation stringency

## Dependencies

Required Python packages:
```
pandas
statsmodels
pingouin
plotly
fkscore
csv (built-in)
```

Install with:
```bash
pip install pandas statsmodels pingouin plotly fkscore
```

## Model Configurations Evaluated

### RAG Configurations
1. **RAGFS** (RAG with Few-Shot): Retrieval + few-shot examples
2. **RAGNFS** (RAG No Few-Shot): Retrieval without examples
3. **NRAG** (No RAG): LLM baseline without retrieval

### Models Tested
- GPT-4 (all three configurations)
- GPT-4o (NRAG and RAGFS)
- GPT-4o-mini (all three configurations)
- GPT-3.5-turbo (RAGFS and RAGNFS)

**Total Configurations:** 10 model-configuration pairs

## Research Questions

This evaluation framework addresses:

1. **Does RAG improve content quality?** Compare RAG vs. NRAG configurations
2. **Do few-shot examples improve outputs?** Compare RAGFS vs. RAGNFS
3. **Which model performs best overall?** Cross-model comparison across metrics
4. **Is there a quality-readability trade-off?** Correlation analysis between metrics
5. **Are evaluations reliable?** ICC and inter-rater agreement assessment

## Findings and Insights

### Performance Patterns (Based on Radar Plot Data)

**Best Overall Performance:**
- GPT-4o_RAGFS: High accuracy (73.6), good completeness (58), excellent readability (95.73)
- GPT-4_RAGFS: Balanced performance across all metrics

**Readability Leaders:**
- GPT-4_RAGFS: 103.33 (above scale max, extremely readable)
- GPT-4o_RAGFS: 95.73

**Accuracy Leaders:**
- GPT-4o_RAGFS: 73.6
- GPT-4omini_RAGNFS: 71.4

**Completeness Leaders:**
- GPT-4omini_NRAG: 60.0
- GPT-4o_RAGFS: 58.0

**Trade-offs Observed:**
- RAG generally improves accuracy and completeness
- NRAG configurations often have lower redundancy
- Few-shot examples tend to improve readability

## Best Practices

### For Evaluation
- Use blind evaluation protocols
- Multiple independent evaluators (minimum 2)
- Standardized scoring rubrics
- Document evaluation criteria clearly
- Calculate inter-rater reliability

### For Analysis
- Combine multiple evaluators for robust statistics
- Use ANOVA to identify significant differences
- Report both mean and standard deviation
- Visualize multi-dimensional performance
- Consider trade-offs between metrics

### For Interpretation
- Higher accuracy and completeness are desirable
- Lower redundancy indicates conciseness
- Target 6th-grade readability for health materials
- Consider context-specific requirements
- Balance multiple objectives (Pareto optimal solutions)

## Future Enhancements

Potential improvements:
- [ ] Automated quality metrics (ROUGE, BLEU, BERTScore)
- [ ] Patient feedback collection
- [ ] Longitudinal effectiveness studies
- [ ] Cost-effectiveness analysis per model
- [ ] A/B testing with real patients
- [ ] Multi-rater evaluation (>2 evaluators)
- [ ] Qualitative analysis of generated content
- [ ] Correlation analysis between metrics

## Related Folders

- [`../langchain_testing/`](../langchain_testing/README.md) - RAG system implementation
- [`../crawling/`](../crawling/README.md) - Source content for knowledge base

## Notes

- Evaluation is labor-intensive and requires domain expertise
- Inter-rater reliability should be established before interpreting results
- Statistical significance doesn't always imply clinical significance
- Patient education materials should be validated with target population
- Readability formulas are estimates; actual comprehension may vary
- Consider health literacy levels of target patient population
- Expert evaluation captures nuances that automated metrics miss

## Citation

When using this evaluation framework for research:
- Report ICC values for inter-rater reliability
- Include both evaluator credentials and training procedures
- Document scoring rubrics and decision rules
- Report statistical methods and significance levels
- Consider pre-registration of analysis plans
